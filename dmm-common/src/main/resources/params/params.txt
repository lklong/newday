1.环境参数
        a.spark作业名称
2.输入
        a.标志(数据源：1，hdfs 2,hive )
        b.hdfs参数：
        "flag":"1",
        "src":["hdfs://test:9000/user/hadoop/data/input/students.txt"],
        "line_parse_regex":"\n",
        "colnum_parse_regex":"\t",
        "fields":[ "1:int","2:String", "5:double", "6:double"]
        "fields_map":["1:2","2:5"]
        c.hive参数
        "flag":"2"
        "query_sql":"select col1,col2 from table
        "fields":[ "col1:int","col2:String"]

3.输出
      a.标志(数据源：1，hdfs 2,hive,3,hdfs&hbase )
      b.hdfs输出
      "flag":"1",
      "store_to_hdfs":{
      "out_path":"hdfs://test:9000/user/hadoop/data/output/stu_0_hdfs",
      "delimeter":"\t",
      "need_head":"1",
      "header":["1","2","5","6"]

      c.hive输出
      "create_tab_sql":"建表sql"
      "store_sql":"插入sql"
4.算法参数

      "algorithm":["params1","params2","params3","..."]
      "params1":"模型数据源"
      "params2":"计算参数"
      "params3":"计算参数"